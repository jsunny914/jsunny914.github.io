<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <title>LIS 500 Project 2 - About Us</title>
    <link rel="stylesheet" href="stylepage.css">
  </head>
  <body>
    <!-- See index.html for comments on 'header', nav, or 'footer'-->
    <div class="header">
      <h1>LIS 500 Project</h1>
      <p>Group: Thomas Reineck, Sarah Schepp, Jyolsna Sunny</p>
    </div>

    <!--Class 'active_tab' indicates this is the about us page-->
    <nav id="menu">
      <a href="index.html">Home</a>
      <a href="aboutus.html" >About Us</a>
      <a href="resources.html">Implicit Bias Resources</a>
      <a href="techhero.html">Our Tech Heroes</a>
      <a href="ml.html" class='active_tab'>ML Project</a>
    </nav>

    <main>
        <div class="mlsection">
            <h1>1. Project Statement and Reflection</h1>
            <p>For our project, we utilized Google’s Teachable Machine platform to attempt to train an image classification model capable of distinguishing between six standard types of polyhedral dice (D4, D6, D8, D10, D12, and D20 named according their number of sides) based solely on their visual profiles. We excluded the D100/D% since it usually has the same shape as the D10. To spoil the ending, this model did not perform well but we did learn valuable lessons along the way.</p>
            <p>One of our goals was to avoid the harms of the “coded gaze,” a term coined and explored by Dr. Joy Buolamwini in her 2023 publication, Unmasking AI: My Mission to Protect What is Human in a World of Machines. For context, this phenomenon is closely related to the “male gaze,” which Buolamwini describes as “a concept developed by media scholars to describe how, in a patriarchal society, art, media, and other forms of representation are created with a male viewer in mind. The male gaze decides which subjects are desirable and worthy of attention, and it determines how they are to be judged” (Buolamwini, 2023, p. 8). In a similar vein, the coded gaze refers to “the ways in which the priorities, preferences, and prejudices of those who have the power to shape technology can propagate harm, such as discrimination and erasure” (Buolamwini, 2023, p. 9). While dice classification seems innocent enough, it could be reflective of more systemic problems with machine learning and specifically image classification.</p>
            <p>Machine learning models often fail to see equitably because their very code stems from human implicit biases embedded in human-acquired and human-processed data (e.g., image, text, sound, etc.). There is no such thing as objective data. For example, if a model is trained only on light-colored dice, the coded gaze means it will only learn about bright dice, excluding dark or patterned ones, even if they are geometrically identical to one another. It is also important to note that Buolamwini’s key project of the Aspire Mirror used facial recognition software created by someone else and it was not until she looked closer into that software that she realized that not recognizing darker faces was a recurring problem. Similarly, it would be impossible for us to diagnose some issues without being able to look under the hood of Google’s Teachable Machines image classification software. We do not know how they are processing the images we give them or what features it will extract.</p>
            <p>With that in mind, our group recognized that building a universal image classification model that could distinguish between all types of dice was inseparable from defeating this sort of bias. If we merely captured photos of one or two sets of dice, the model would not learn fundamental geometrical differences, but instead identify correlations that are insignificant to our main goal. Therefore, to promote model recognition of all dice, we set out to upload as many examples of different dice (different angles, colors, textures, etc.) to demonstrate that Buolamwini’s ethical guidance is an effective lesson in trying to achieve representative algorithmic function.</p>
            <p>That said, this process was not simple, and we did encounter obstacles. For example, the model had difficulty distinguishing between the D8 and D10 as well as the D12 and the D20, likely due to their geometric similarities. However, these failures were a useful educational moment, and served as a low-stakes manifestation of the coded gaze. Throughout her text, Buolamwini warns that AI systems, being pattern seekers, will always prioritize patterns that reinforce existing biases present in training data. In our case, the pattern did not involve human identity, but geometric identity. The model struggled because the initial training data was insufficient, allowing it to latch onto any misleading visual cues.</p>
            <p>We uploaded approximately 90 pictures of unique dice (about 15 in each classification) in an attempt to train the model to identify geometric differences between items, rather than relying on irrelevant features such as color. Google Teachable Machines has camera recording capabilities that would then take each frame and add it to the training data set. We however did not use this feature, choosing instead to capture individual photos. Recording a still object would mean hundreds of the same photo which would skew the data. Recording an object that we would move by hand meant our hand would obscure some images, causing more features to come into play and affecting the training. So we opted to take pictures of dice sitting on a table, which also meant there were not too many angles that we could take pictures from. As a result of these limitations, we had a very small but varied data set.</p>
            <p>Despite our attempts in creating a universal classification model and combating problems we knew would come up, the model did not perform with a high accuracy. Looking under the hood of the results, the loss on the testing data set was increasing after approximately 10 epochs and we trained the model for 50 epochs. This meant that the model was overfitting. In other words, it was a bit more focused on the specific dice pictures we had given it as opposed to learning patterns that could be attributed to all dice. Training the model for less epochs or adding significantly more data (such that 50 epochs would be a more reasonable number) would likely result in a more accurate model. It is also worth noting that the smallest batch size we could select was 16, which when you have 16 or less images per classification, is too large a batch size. The model may select only one classification in one of its batches, which would heavily skew the training. The potential solution, again, is more data, and not just any data, but representative data.</p>
            <p>This classification project, viewed through the lens of Joy Buolamwini’s Unmasking AI, demonstrates that creating a functional machine learning model is inseparable from professional ethical diligence. Despite the low accuracy of our model, its successes were a direct result of proactively confronting the coded gaze that manifested in our experimentation. Data curation is an important part of this ethical diligence, but so is data processing. Google’s Teachable Machines hides that very important step from the user, creating even more of black box that machine learning usually is for the developer. Knowing the features calculated and the weights attributed to those features may help in identifying more issues in our data and help us create a more robust model. At the end of the day, even a dice classification system is not immune to the coded gaze.</p>
            <p>The coded gaze propagates harm because those who create programs have their own biases and prejudices, conscious or unconscious, that are replicated in the code of their creations. As such, it is all the more important for programmers to work ethically with software, even when it comes to small-scale, low-potential-for-harm projects like dice recognition software. This sentiment even extends to us as students, as even Buolamwini put it early on in her work, “I still hoped someone else would take care of the problem. I was in school, and I enjoyed the privilege of creating technology without thinking of consequences and social problems” (Buolamwini, 2023, p. 11). As AI, particularly generative AI, becomes more commonplace in today’s society, it is important to realize how much our own work in this course has shaped our mindset, and how these projects, even as simple as identifying dice, can have an impact on others.</p>
            <p>References</p>
            <p>Buolamwini, J. (2023). <i>Unmasking AI.</i> Random House.</p>
          </div>
      
<div class="mlsection">
    <h2>Visual Guide & Model Output Clarification</h2>
    <figure>
        <img src="dice_legend.png" alt="Visual reference guide for polyhedral dice, showing one of each type labeled D4, D6, D8, D10, D12, and D20." style="max-width: 70%; height: auto; display: block; margin: 0 auto;">
        <figcaption style="text-align: center;">Figure 1: Visual Legend for the Six Polyhedral Dice Classes.</figcaption>
    </figure>
    <p><strong>Note on Model Output:</strong> The number shown next to the die type (e.g., 0.97) represents the model's confidence level, or probability, that the image provided belongs to that specific classification. Scores closer to 1.00 indicate higher certainty.</p>
</div>
        <div class="mlsection">
          <h1>2. Project Setup</h1>
            <div class="img_container"><img src="mlscreenshot.png"/></div>      
        </div>

        <div class="mlsection">
          <h1>3. Project Demo</h1>
          <div class="video_container"> 
            <iframe width="560" height="315" src="https://www.youtube.com/embed/76hbfxdMnYY?si=QTq_ZYNVU32EknJ6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>    
          </div>
        </div>

        <div class="mlsection">
        <h1>4. <a href="https://teachablemachine.withgoogle.com/models/v4XwEPqaH/">Try our Teachable Machine Project here!</a></h1>
          <div class="livedemo">
            <!-- This snippet of code is copied and pasted from Google Teachable Machines. The only edit I made is filling in the URL and adding styling on the stylepage. -->
            <div>Teachable Machine Image Model</div>
            <button type='button' onclick='init()'>Start</button>
            <div id='webcam-container'></div>
            <div id='label-container'></div>
            <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.3/dist/teachablemachine-image.min.js"></script>
            <script type="text/javascript">
                // More API functions here:
                // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

                // the link to your model provided by Teachable Machine export panel
                const URL = 'https://teachablemachine.withgoogle.com/models/v4XwEPqaH/';

                let model, webcam, labelContainer, maxPredictions;

                let isIos = false; 
                // fix when running demo in ios, video will be frozen;
                if (window.navigator.userAgent.indexOf('iPhone') > -1 || window.navigator.userAgent.indexOf('iPad') > -1) {
                  isIos = true;
                }
                // Load the image model and setup the webcam
                async function init() {
                    const modelURL = URL + 'model.json';
                    const metadataURL = URL + 'metadata.json';

                    // load the model and metadata
                    // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
                    // or files from your local hard drive
                    model = await tmImage.load(modelURL, metadataURL);
                    maxPredictions = model.getTotalClasses();

                    // Convenience function to setup a webcam
                    const flip = true; // whether to flip the webcam
                    const width = 200;
                    const height = 200;
                    webcam = new tmImage.Webcam(width, height, flip);
                    await webcam.setup(); // request access to the webcam

                    if (isIos) {
                        document.getElementById('webcam-container').appendChild(webcam.webcam); // webcam object needs to be added in any case to make this work on iOS
                        // grab video-object in any way you want and set the attributes
                        const webCamVideo = document.getElementsByTagName('video')[0];
                        webCamVideo.setAttribute("playsinline", true); // written with "setAttribute" bc. iOS buggs otherwise
                        webCamVideo.muted = "true";
                        webCamVideo.style.width = width + 'px';
                        webCamVideo.style.height = height + 'px';
                    } else {
                        document.getElementById("webcam-container").appendChild(webcam.canvas);
                    }
                    // append elements to the DOM
                    labelContainer = document.getElementById('label-container');
                    for (let i = 0; i < maxPredictions; i++) { // and class labels
                        labelContainer.appendChild(document.createElement('div'));
                    }
                    webcam.play();
                    window.requestAnimationFrame(loop);
                }

                async function loop() {
                    webcam.update(); // update the webcam frame
                    await predict();
                    window.requestAnimationFrame(loop);
                }

                // run the webcam image through the image model
                async function predict() {
                    // predict can take in an image, video or canvas html element
                    let prediction;
                    if (isIos) {
                        prediction = await model.predict(webcam.webcam);
                    } else {
                        prediction = await model.predict(webcam.canvas);
                    }
                    for (let i = 0; i < maxPredictions; i++) {
                        const classPrediction =
                            prediction[i].className + ': ' + prediction[i].probability.toFixed(2);
                        labelContainer.childNodes[i].innerHTML = classPrediction;
                    }
                }
            </script>
            <!-- End copy pasted code from Google Teachable Machines -->
          </div>
        </div>
    </main>

    <div class="footer">
      
    </div>
  
  </body>
</html>
