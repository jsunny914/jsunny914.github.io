<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <title>LIS 500 Project 2 - About Us</title>
    <link rel="stylesheet" href="stylepage.css">
  </head>
  <body>
    <!-- See index.html for comments on 'header', nav, or 'footer'-->
    <div class="header">
      <h1>LIS 500 Project</h1>
      <p>Group: Thomas Reineck, Sarah Schepp, Jyolsna Sunny</p>
    </div>

    <!--Class 'active_tab' indicates this is the about us page-->
    <nav id="menu">
      <a href="index.html">Home</a>
      <a href="aboutus.html" >About Us</a>
      <a href="resources.html">Implicit Bias Resources</a>
      <a href="techhero.html">Our Tech Heroes</a>
      <a href="ml.html" class='active_tab'>ML Project</a>
    </nav>

    <main>
        <div class="mlsection">
            <h1>1. Project Statement</h1>
            <p>For our project, we utilized Google’s Teachable Machine platform to train an image classification model capable of distinguishing between the six standard types of polyhedral dice (D4, D6, D8, D10, D12, and D20) based solely on their visual profiles. Our initial goal was simple: to achieve universal classification across these six distinct shapes to avoid the harms of the “coded gaze,” a term coined and explored by Dr. Joy Buolamwini in her 2023 publication, Unmasking AI: My Mission to Protect What is Human in a World of Machines. For context, this phenomenon is closely related to the “male gaze,” which Buolamwini describes as “a concept developed by media scholars to describe how, in a patriarchal society, art, media, and other forms of representation are created with a male viewer in mind. The male gaze decides which subjects are desirable and worthy of attention, and it determines how they are to be judged” (Buolamwini, 2023, p. 8). In a similar vein, the coded gaze refers to “the ways in which the priorities, preferences, and prejudices of those who have the power to shape technology can propagate harm, such as discrimination and erasure” (Buolamwini, 2023, p. 9).</p>
            <p>Machine Learning models like “AI” often fail to see equitably because their very code stems from human implicit biases embedded in human-acquired training data (e.g., image, text, sound, etc.). For example, if a model is trained only on light-colored dice, the coded gaze means it will only “see” bright dice, excluding dark or patterned ones, even if they are geometrically identical to one another. With that in mind, our group recognized that building a functional image classification model that could distinguish between all types of dice was inseparable from defeating this sort of bias. If we merely captured photos of one or two sets of dice, the model would not learn fundamental geometrical differences, but instead identify biased correlations (e.g., “A D4 is a yellow cube”). So, to ensure model recognition of all dice, we set out to upload as many examples of different dice (different angles, colors, sides, features) to demonstrate that Buolamwini’s ethical guidance is an effective way to achieve representative algorithmic function.</p>
            <p>That said, this process was not simple, and we did in fact encounter obstacles, chief among them being that our model could not distinguish between certain dice, namely the D12 and D20. However, these failures were a useful educational moment, and served as a low-stakes manifestation of the coded gaze. Indeed, throughout her text, Buolamwini warns that AI systems, being pattern seekers, will always prioritize patterns that reinforce existing biases present in training data. In our case, the “pattern” did not involve human identity, but geometric identity. The model struggled because the initial training data (which was provided manually, through individual photo uploads) was insufficient, allowing it to latch onto any misleading visual cues.</p>
            <p>For instance, the original 72 photos uploaded to the program were only shot from a singular angle - even if the dice sets themselves were diverse in coloration, features, etc. So, when we tested the original model on dice that varied slightly (in this case, D12 and D20), it could not confidently distinguish between them. Though, on paper, we represented each type of dice equally, this failure highlighted the need for a more diversified data set to reduce and/or eliminate such misinterpretations in how the dice were represented in the uploaded images.</p>
            <p>So, to confront these shortcomings, we added more photos of different dice from different angles. This ensured the model was identifying geometric differences between items, rather than relying on superficial correlations (e.g., color). This was done using the Google Teachable Machine’s camera capabilities, allowing for the easy capturing of individual dice from hundreds of different angles. Each type of dice received this treatment, which prevented any single die type from becoming a statistically dominant “norm” that could adversely influence the model’s perceptions. All in all, this strategy proved not only to be more efficient (taking less time than capturing images manually), but more ethically and functionally effective as well. </p>
            <p>This classification project, viewed through the lens of Joy Buolamwini’s Unmasking AI, demonstrates that creating a functional machine learning model is inseparable from professional ethical diligence. The high-accuracy of our dice recognition model was not because the machine had some innate ability to differentiate between different objects. Rather, it was a direct result of proactively confronting the coded gaze that manifested in our initial aforementioned failure. Our early failure served as a reminder of Buolamwini’s warnings: that AI will always seek the path of least resistance if they are not forced to learn. In our case, the original data consisted of dice photos that only captured a singular angle (top-down), so the model failed to learn the geometry of the different objects. This invariably led to the model’s consistent struggle differentiating between D12 and D20 dice.</p>
            <p>To counteract the bias inherent in the data, we started over and injected images of every die type from hundreds of angles and orientations. This process forced the model to learn the innate geometric characteristics of each die, leading to a more accurate image-recognition program. And of course, this later success proved the value in heading Buolamwini’s argument that the harms of the coded gaze - “discrimination and erasure”  (Buolamwini, 2023, p. 9).  - are directly caused by human failure to remain diligent in data curation. This, of course, suggests that the meticulous efforts required to ensure that all dice are recognized equitably, independent of hue or texture, color or otherwise, is directly analogous with the effort required to build equitable generative models like Gemini, or ChatGPT.</p>
            <p>Indeed, as stated earlier in this essay, the coded gaze propagates harm because those who create programs have their own biases and prejudices, conscious or unconscious, that are replicated in the code of their creations. As such, it is all the more important for programmers to work ethically with software, even when it comes to small-scale, low-potential-for-harm projects like dice recognition software. This sentiment even extends to us as students, as even Buolamwini put it early on in her work, “I still hoped someone else would take care of the problem. I was in school, and I enjoyed the privilege of creating technology without thinking of consequences and social problems” (Buolamwini, 2023, p. 11). As AI, particularly generative AI, becomes more commonplace in today’s society, it is important to realize how much our own work in this course has shaped our mindset, and how these projects, even as simple as identifying dice, can have an impact on others.</p>
            <p>References</p>
            <p>Buolamwini, J. (2023). <i>Unmasking AI.</i> Random House.</p>
          </div>

        <div class="mlsection">
          <h1>2. Project</h1>
            <div><a href="https://teachablemachine.withgoogle.com/models/v4XwEPqaH/">Try our Teachable Machine Project here!</a></div>
            <div><img src="mlscreenshot.png"/></div>      
        </div>

        <div>Teachable Machine Image Model</div>
        <button type='button' onclick='init()'>Start</button>
        <div id='webcam-container'></div>
        <div id='label-container'></div>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.3/dist/teachablemachine-image.min.js"></script>
        <script type="text/javascript">
            // More API functions here:
            // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

            // the link to your model provided by Teachable Machine export panel
            const URL = './mlfiles/';

            let model, webcam, labelContainer, maxPredictions;

            let isIos = false; 
            // fix when running demo in ios, video will be frozen;
            if (window.navigator.userAgent.indexOf('iPhone') > -1 || window.navigator.userAgent.indexOf('iPad') > -1) {
              isIos = true;
            }
            // Load the image model and setup the webcam
            async function init() {
                const modelURL = URL + 'model.json';
                const metadataURL = URL + 'metadata.json';

                // load the model and metadata
                // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
                // or files from your local hard drive
                model = await tmImage.load(modelURL, metadataURL);
                maxPredictions = model.getTotalClasses();

                // Convenience function to setup a webcam
                const flip = true; // whether to flip the webcam
                const width = 200;
                const height = 200;
                webcam = new tmImage.Webcam(width, height, flip);
                await webcam.setup(); // request access to the webcam

                if (isIos) {
                    document.getElementById('webcam-container').appendChild(webcam.webcam); // webcam object needs to be added in any case to make this work on iOS
                    // grab video-object in any way you want and set the attributes
                    const webCamVideo = document.getElementsByTagName('video')[0];
                    webCamVideo.setAttribute("playsinline", true); // written with "setAttribute" bc. iOS buggs otherwise
                    webCamVideo.muted = "true";
                    webCamVideo.style.width = width + 'px';
                    webCamVideo.style.height = height + 'px';
                } else {
                    document.getElementById("webcam-container").appendChild(webcam.canvas);
                }
                // append elements to the DOM
                labelContainer = document.getElementById('label-container');
                for (let i = 0; i < maxPredictions; i++) { // and class labels
                    labelContainer.appendChild(document.createElement('div'));
                }
                webcam.play();
                window.requestAnimationFrame(loop);
            }

            async function loop() {
                webcam.update(); // update the webcam frame
                await predict();
                window.requestAnimationFrame(loop);
            }

            // run the webcam image through the image model
            async function predict() {
                // predict can take in an image, video or canvas html element
                let prediction;
                if (isIos) {
                    prediction = await model.predict(webcam.webcam);
                } else {
                    prediction = await model.predict(webcam.canvas);
                }
                for (let i = 0; i < maxPredictions; i++) {
                    const classPrediction =
                        prediction[i].className + ': ' + prediction[i].probability.toFixed(2);
                    labelContainer.childNodes[i].innerHTML = classPrediction;
                }
            }
        </script>

    </main>

    <div class="footer">
      
    </div>
  
  </body>
</html>